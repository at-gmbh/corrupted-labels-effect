{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# silence tensorflow deprecation warnings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# add src to sys.path and import local modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import src.false_labels_effect.callbacks as cbs \n",
    "import src.false_labels_effect.data_loader as dl\n",
    "import src.false_labels_effect.models as mdls\n",
    "import src.false_labels_effect.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> TODO: set parameter below <--\n",
    "\n",
    "# Select model task\n",
    "#   'Class': main class (4) classification\n",
    "#   'Subclass': sub class (14) classification\n",
    "#   'Annotations' : polygon vertices prediction\n",
    "model_task = 'Class'\n",
    "\n",
    "# set number of classes in labels\n",
    "if model_task.lower() == 'class':\n",
    "    n_classes = 4\n",
    "elif model_task.lower() == 'subclass':\n",
    "    n_classes = 14\n",
    "\n",
    "# set number of images\n",
    "limit_loaded_images = None  # use None for \"all\" images\n",
    "\n",
    "# set target size of images\n",
    "resize_to = (244, 244) \n",
    "\n",
    "# set list of ratio of false labels in training data between 0 and 1 (low to high)\n",
    "# 0.05 => 5% of images in training data will be labeled incorrectly\n",
    "false_ratios = [0.05]\n",
    "\n",
    "# define data loader parameters\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# define model processing parameter\n",
    "n_epochs = 10\n",
    "multiprocessing = True\n",
    "n_workers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training and test img png path\n",
    "train_img_png_path = Path('../data/Images_4c_Poly/Train')\n",
    "test_img_png_path = Path('../data/Images_4c_Poly/Test')\n",
    "\n",
    "# set training and test img npy path\n",
    "train_img_npy_path = Path('../data/Images_4c_Poly/Train_npy')\n",
    "test_img_npy_path = Path('../data/Images_4c_Poly/Test_npy')\n",
    "\n",
    "# set label path\n",
    "train_label_path = Path('../data/Labels_4c_Poly')\n",
    "test_label_path = Path('../data/Labels_4c_Poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "train_labels_dict = util.load_labels(f'{train_label_path}/Train.npy')\n",
    "test_labels_dict = util.load_labels(f'{test_label_path}/Test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train images, resize and save as npy\n",
    "if not os.path.exists(f'{train_img_npy_path}'):\n",
    "    os.mkdir(train_img_npy_path)\n",
    "\n",
    "    i = 0\n",
    "    for image_path in train_img_png_path.iterdir():\n",
    "        i += 1\n",
    "        if limit_loaded_images is not None and i > limit_loaded_images:\n",
    "            break\n",
    "\n",
    "        # Load without resizing so that polygon fits (for now)\n",
    "        img_id = image_path.name.split(\".\")[0]\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Use util resize function resize image and polygon\n",
    "        # TODO: poly resize currently not saved\n",
    "        img_res, poly_res = util.resize(\n",
    "            img, train_labels_dict[img_id], resize_to\n",
    "        )\n",
    "\n",
    "        npy_img = img_to_array(img_res)\n",
    "        np.save(f'{train_img_npy_path}/{img_id}', npy_img)\n",
    "\n",
    "# load test images, resize and save as npy\n",
    "if not os.path.exists(f'{test_img_npy_path}'):\n",
    "    os.mkdir(test_img_npy_path)\n",
    "    i = 0\n",
    "\n",
    "    for image_path in test_img_png_path.iterdir():\n",
    "        i += 1\n",
    "        if limit_loaded_images is not None and i > limit_loaded_images:\n",
    "            break\n",
    "\n",
    "        # Load without resizing so that polygon fits (for now)\n",
    "        img_id = image_path.name.split(\".\")[0]\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Use util resize function resize image and polygon\n",
    "        # TODO: poly resize currently not saved\n",
    "        img_res, poly_res = util.resize(\n",
    "            img, test_labels_dict[img_id], resize_to\n",
    "        )\n",
    "\n",
    "        npy_img = img_to_array(img_res)\n",
    "        np.save(f'{test_img_npy_path}/{img_id}', npy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of included train and test images, format for keras data loader\n",
    "partition = {}\n",
    "train_img_ids_included = [str(i.name).split(\".\")[0] for i in train_img_npy_path.iterdir()]\n",
    "test_img_ids_included = [str(i.name).split(\".\")[0] for i in test_img_npy_path.iterdir()]\n",
    "\n",
    "# split for test and train\n",
    "partition['train'] = [id for id in train_img_ids_included if 'Train' in id]\n",
    "partition['test'] = [id for id in test_img_ids_included if 'Test' in id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Blob - All\n",
      "1: Blob - Blue\n",
      "2: Blob - Green\n",
      "3: Blob - Red\n",
      "4: Blur - None\n",
      "5: Channel_Change - ['Blue', 'Green', 'Red']\n",
      "6: Channel_Change - ['Blue', 'Red', 'Green']\n",
      "7: Channel_Change - ['Green', 'Blue', 'Red']\n",
      "8: Channel_Change - ['Green', 'Red', 'Blue']\n",
      "9: Channel_Change - ['Red', 'Blue', 'Green']\n",
      "10: Distortion - All\n",
      "11: Distortion - Blue\n",
      "12: Distortion - Green\n",
      "13: Distortion - Red\n"
     ]
    }
   ],
   "source": [
    "# filter train labels to only include transformed images\n",
    "train_labels_dict_incl = {}\n",
    "for (key, value) in train_labels_dict.items():\n",
    "    if key in partition['train']:\n",
    "        train_labels_dict_incl[key] = value\n",
    "\n",
    "# filter test labels to only include transformed images\n",
    "test_labels_dict_incl = {}\n",
    "for (key, value) in test_labels_dict.items():\n",
    "    if key in partition['test']:\n",
    "        test_labels_dict_incl[key] = value\n",
    "\n",
    "# generate flattened dict of model task corresponding labels\n",
    "train_labels_dict_flat = util.select_label(train_labels_dict_incl, model_task)\n",
    "test_labels_dict_flat = util.select_label(test_labels_dict_incl, model_task)\n",
    "\n",
    "# encode categorical labels for classification tasks\n",
    "if model_task in ['Class', 'Subclass']:\n",
    "    label_mapping, y_train, y_test, = util.encode_labels(train_labels_dict_flat, test_labels_dict_flat)\n",
    "    \n",
    "    label_mapping = {int(k):str(v) for k, v in label_mapping.items()}\n",
    "    for k, v in label_mapping.items():\n",
    "        print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train imgs: 240 - # val imgs: 60 - # test imgs: 300\n"
     ]
    }
   ],
   "source": [
    "# split training data into train and validation\n",
    "partition['train'], y_train, partition['val'], y_val = util.train_val_split(partition['train'], y_train, val_split)\n",
    "print('# train imgs:', len(partition['train']), '- # val imgs:', len(partition['val']), '- # test imgs:', len(partition['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create false train labels for all given ratios\n",
    "util.make_false_labels(train_label_path, y_train, false_ratios, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false labels: 12\n",
      "Model: \"resnet_00500r_14c\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 8, 8, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 14)                28686     \n",
      "=================================================================\n",
      "Total params: 27,812,750\n",
      "Trainable params: 27,759,630\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "None\n",
      "7/7 - 105s - loss: 3.6433 - accuracy: 0.2902 - val_loss: 17242.9648 - val_accuracy: 0.2812\n",
      "resnet_00500r_14c - Test accuracy: 0.2604166567325592 \n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ratio in false_ratios:\n",
    "    # load false train labels\n",
    "    y_train = util.load_false_labels(train_label_path, ratio)\n",
    "\n",
    "    # define data loader parameters\n",
    "    params = {'dim': (resize_to[0],resize_to[1]),\n",
    "              'batch_size': batch_size,\n",
    "              'n_classes': n_classes,\n",
    "              'n_channels': 3,\n",
    "              'shuffle': True}\n",
    "\n",
    "    # load data\n",
    "    training_loader = dl.DataLoader(partition['train'], y_train, **params)\n",
    "    validation_loader = dl.DataLoader(partition['val'], y_val, **params)\n",
    "    test_loader = dl.DataLoader(partition['test'], y_test, **params)\n",
    "\n",
    "    # load models\n",
    "    basic_cnn = mdls.create_cnn_model(resize_to, n_classes, ratio)\n",
    "    resnet_cnn = mdls.create_resnet_model(resize_to, n_classes, ratio)\n",
    "    all_models = [resnet_cnn] # TODO: set models to be included\n",
    "\n",
    "    for model in all_models:\n",
    "        print(model.summary())\n",
    "\n",
    "        model_start_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "        # initialize logging\n",
    "        logdir_scalars = f'../logs/scalars/{model._name}/{model_start_time}'\n",
    "        logdir_label_mapper = f'../logs/label_mapping/{model._name}/{model_start_time}'\n",
    "        \n",
    "        os.makedirs(logdir_label_mapper)\n",
    "        with open(logdir_label_mapper + '/label_mapper.json', 'w+') as f:\n",
    "            json.dump(label_mapping, f)\n",
    "\n",
    "        tensorboard_callback = TensorBoard(log_dir=logdir_scalars)\n",
    "        classReport_callback = cbs.class_report_cb(test_loader, model_start_time)\n",
    "\n",
    "        file_writer = tf.summary.create_file_writer(logdir_scalars)\n",
    "\n",
    "        # compile model\n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # train model with tensorboard and classification report logging\n",
    "        history = model.fit(x = training_loader,\n",
    "                                epochs = n_epochs,\n",
    "                                verbose = 2,\n",
    "                                callbacks=[classReport_callback,\n",
    "                                           tensorboard_callback],\n",
    "                                validation_data = validation_loader,\n",
    "                                use_multiprocessing = multiprocessing,\n",
    "                                workers = n_workers)\n",
    "        \n",
    "        # show test accuracy\n",
    "        score = model.evaluate(x = test_loader,\n",
    "                               callbacks=[tensorboard_callback],\n",
    "                               use_multiprocessing = multiprocessing,\n",
    "                               workers = n_workers,\n",
    "                               verbose = 0)\n",
    "\n",
    "        print(model._name, '- Test accuracy:', score[1],\n",
    "              f'\\n{\"=\" * 65}\\n')\n",
    "\n",
    "        model.save(f'../logs/models/{model._name}/{model_start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up false labels files\n",
    "for ratio in false_ratios:\n",
    "    os.remove(f'{train_label_path}/Train_{format(int(ratio*10000),\"05d\")}r.npy')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "785473fea46f9ac689a96b3810ed86c0f03e438ffd4c73e9fe241b9bd8d70125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('false-labels': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
