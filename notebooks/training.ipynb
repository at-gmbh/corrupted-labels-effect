{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import src.false_labels_effect.util as util\n",
    "import src.false_labels_effect.data_loader as dl\n",
    "import src.false_labels_effect.models as mdls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select model task\n",
    "#   'Class': main class classification\n",
    "#   'Subclass': sub class classification\n",
    "#   'Annotations' : polygon vertices prediction\n",
    "model_task = 'Class'\n",
    "\n",
    "# TODO: set number of images\n",
    "limit_loaded_images = 10  # use None for \"all\" images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training and test img png path\n",
    "train_img_png_path = Path(\"..\\\\data\\\\Images_4c_Poly\\\\Train\")\n",
    "test_img_png_path = Path(\"..\\\\data\\\\Images_4c_Poly\\\\Test\")\n",
    "\n",
    "# set training and test img npy path\n",
    "img_npy_path = Path('..\\\\data\\\\Images_4c_Poly\\\\Train_Test_npy')\n",
    "\n",
    "# set label path\n",
    "train_label_path = Path(\"..\\\\data\\\\Labels_4c_Poly\\\\Train.npy\")\n",
    "test_label_path = Path(\"..\\\\data\\\\Labels_4c_Poly\\\\Test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load labels dict\n",
    "train_labels_dict = util.load_labels(train_label_path)\n",
    "test_labels_dict = util.load_labels(test_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set size of images \n",
    "resize_to = (244, 244)\n",
    "\n",
    "# load train images, resize and save as npy\n",
    "if not os.path.exists(f'{img_npy_path}'):\n",
    "    os.mkdir(img_npy_path)\n",
    "\n",
    "    i = 0\n",
    "    for image_path in train_img_png_path.iterdir():\n",
    "        i += 1\n",
    "        if limit_loaded_images is not None and i > limit_loaded_images:\n",
    "            break\n",
    "\n",
    "        # Load without resizing so that polygon fits (for now)\n",
    "        img_id = image_path.name.split(\".\")[0]\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Use util resize function resize image and polygon\n",
    "        # TODO: poly resize currently not saved\n",
    "        img_res, poly_res = util.resize(\n",
    "            img, train_labels_dict[img_id], resize_to\n",
    "        )\n",
    "\n",
    "        npy_img = img_to_array(img_res)\n",
    "        np.save(f'{img_npy_path}\\\\{img_id}', npy_img)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for image_path in test_img_png_path.iterdir():\n",
    "        i += 1\n",
    "        if limit_loaded_images is not None and i > limit_loaded_images:\n",
    "            break\n",
    "\n",
    "        # Load without resizing so that polygon fits (for now)\n",
    "        img_id = image_path.name.split(\".\")[0]\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Use util resize function resize image and polygon\n",
    "        # TODO: poly resize currently not saved\n",
    "        img_res, poly_res = util.resize(\n",
    "            img, test_labels_dict[img_id], resize_to\n",
    "        )\n",
    "\n",
    "        npy_img = img_to_array(img_res)\n",
    "        np.save(f'{img_npy_path}\\\\{img_id}', npy_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of included train and test images formated for keras data loader\n",
    "partition = {}\n",
    "img_ids_included = [str(i.name).split(\".\")[0] for i in img_npy_path.iterdir()]\n",
    "partition['train'] = [id for id in img_ids_included if 'Train' in id]\n",
    "partition['test'] = [id for id in img_ids_included if 'Test' in id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter train labels to only include transformed images\n",
    "train_labels_dict_incl = {}\n",
    "for (key, value) in train_labels_dict.items():\n",
    "    if key in partition['train']:\n",
    "        train_labels_dict_incl[key] = value\n",
    "\n",
    "# filter test labels to only include transformed images\n",
    "test_labels_dict_incl = {}\n",
    "for (key, value) in test_labels_dict.items():\n",
    "    if key in partition['test']:\n",
    "        test_labels_dict_incl[key] = value\n",
    "\n",
    "# generate flattened dict of model task corresponding labels\n",
    "train_labels_dict_flat = util.select_label(train_labels_dict_incl, model_task)\n",
    "test_labels_dict_flat = util.select_label(test_labels_dict_incl, model_task)\n",
    "\n",
    "# encode categorical labels for classification tasks\n",
    "if model_task in ['Class', 'Subclass']:\n",
    "    train_labels_dict_flat, test_labels_dict_flat = util.encode_labels(train_labels_dict_flat, test_labels_dict_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loader parameters\n",
    "batch_size = 10\n",
    "\n",
    "params = {'dim': (resize_to[0],resize_to[0]),\n",
    "          'batch_size': batch_size,\n",
    "          'n_classes': 4,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# load data\n",
    "training_loader = dl.DataLoader(partition['train'], train_labels_dict_flat, **params)\n",
    "test_loader = dl.DataLoader(partition['test'], test_labels_dict_flat, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.9407 - accuracy: 0.2000\n",
      "Epoch 2/5\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48689.6523 - accuracy: 0.3000\n",
      "Epoch 3/5\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10904.3330 - accuracy: 0.4000\n",
      "Epoch 4/5\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1120.6929 - accuracy: 0.1000\n",
      "Epoch 5/5\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1033.3845 - accuracy: 0.3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23a00913f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model parameter\n",
    "multiprocessing = True\n",
    "n_workers = 2\n",
    "\n",
    "# load models\n",
    "basic_cnn = mdls.create_cnn_model(resize_to)\n",
    "\n",
    "# compile models and train\n",
    "basic_cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "basic_cnn.fit(x = training_loader,\n",
    "            #   validation_data = test_loader,\n",
    "              epochs = 5,\n",
    "              use_multiprocessing = multiprocessing,\n",
    "              workers = n_workers,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Test accuracy: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "# show test accuracy\n",
    "score = basic_cnn.evaluate(x = test_loader,\n",
    "                           batch_size = batch_size,\n",
    "                           use_multiprocessing = multiprocessing,\n",
    "                           workers = n_workers,\n",
    "                           verbose = 0)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "785473fea46f9ac689a96b3810ed86c0f03e438ffd4c73e9fe241b9bd8d70125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('false-labels': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
