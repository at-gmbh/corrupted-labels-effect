{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# silence tensorflow deprecation warnings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# add src to sys.path and import local modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import src.false_labels_effect.callbacks as cbs \n",
    "import src.false_labels_effect.data_loader as dl\n",
    "import src.false_labels_effect.models as mdls\n",
    "import src.false_labels_effect.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> TODO: set parameter below <--\n",
    "\n",
    "# Select model task\n",
    "#   'Class': main class (4) classification\n",
    "#   'Subclass': sub class (14) classification\n",
    "#   'Annotations' : polygon vertices prediction\n",
    "model_task = 'Class'\n",
    "\n",
    "# set number of classes in labels\n",
    "if model_task == 'Class':\n",
    "    n_classes = 4\n",
    "elif model_task == 'Subclass':\n",
    "    n_classes = 14\n",
    "\n",
    "# set number of images\n",
    "limit_loaded_images = 300  # use None for \"all\" images\n",
    "\n",
    "# set target size of images\n",
    "resize_to = (244, 244) \n",
    "\n",
    "# set list of ratio of false labels in training data\n",
    "false_ratio = [0.25]\n",
    "\n",
    "# define data loader parameters\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# define model processing parameter\n",
    "n_epochs = 2\n",
    "multiprocessing = False\n",
    "n_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training and test img png path\n",
    "train_img_png_path = Path('..\\\\data\\\\Images_4c_Poly\\\\Train')\n",
    "test_img_png_path = Path('..\\\\data\\\\Images_4c_Poly\\\\Test')\n",
    "\n",
    "# set training and test img npy path\n",
    "train_img_npy_path = Path('..\\\\data\\\\Images_4c_Poly\\\\Train_npy')\n",
    "test_img_npy_path = Path('..\\\\data\\\\Images_4c_Poly\\\\Test_npy')\n",
    "\n",
    "# set label path\n",
    "train_label_path = Path('..\\\\data\\\\Labels_4c_Poly')\n",
    "test_label_path = Path('..\\\\data\\\\Labels_4c_Poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "train_labels_dict = util.load_labels(f'{train_label_path}\\Train.npy')\n",
    "test_labels_dict = util.load_labels(f'{test_label_path}\\Test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train images, resize and save as npy\n",
    "if not os.path.exists(f'{train_img_npy_path}'):\n",
    "    os.mkdir(train_img_npy_path)\n",
    "\n",
    "    i = 0\n",
    "    for image_path in train_img_png_path.iterdir():\n",
    "        i += 1\n",
    "        if limit_loaded_images is not None and i > limit_loaded_images:\n",
    "            break\n",
    "\n",
    "        # Load without resizing so that polygon fits (for now)\n",
    "        img_id = image_path.name.split(\".\")[0]\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Use util resize function resize image and polygon\n",
    "        # TODO: poly resize currently not saved\n",
    "        img_res, poly_res = util.resize(\n",
    "            img, train_labels_dict[img_id], resize_to\n",
    "        )\n",
    "\n",
    "        npy_img = img_to_array(img_res)\n",
    "        np.save(f'{train_img_npy_path}\\\\{img_id}', npy_img)\n",
    "\n",
    "# load test images, resize and save as npy\n",
    "if not os.path.exists(f'{test_img_npy_path}'):\n",
    "    os.mkdir(test_img_npy_path)\n",
    "    i = 0\n",
    "\n",
    "    for image_path in test_img_png_path.iterdir():\n",
    "        i += 1\n",
    "        if limit_loaded_images is not None and i > limit_loaded_images:\n",
    "            break\n",
    "\n",
    "        # Load without resizing so that polygon fits (for now)\n",
    "        img_id = image_path.name.split(\".\")[0]\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Use util resize function resize image and polygon\n",
    "        # TODO: poly resize currently not saved\n",
    "        img_res, poly_res = util.resize(\n",
    "            img, test_labels_dict[img_id], resize_to\n",
    "        )\n",
    "\n",
    "        npy_img = img_to_array(img_res)\n",
    "        np.save(f'{test_img_npy_path}\\\\{img_id}', npy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of included train and test images, format for keras data loader\n",
    "partition = {}\n",
    "train_img_ids_included = [str(i.name).split(\".\")[0] for i in train_img_npy_path.iterdir()]\n",
    "test_img_ids_included = [str(i.name).split(\".\")[0] for i in test_img_npy_path.iterdir()]\n",
    "\n",
    "# split for test and train\n",
    "partition['train'] = [id for id in train_img_ids_included if 'Train' in id]\n",
    "partition['test'] = [id for id in test_img_ids_included if 'Test' in id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter train labels to only include transformed images\n",
    "train_labels_dict_incl = {}\n",
    "for (key, value) in train_labels_dict.items():\n",
    "    if key in partition['train']:\n",
    "        train_labels_dict_incl[key] = value\n",
    "\n",
    "# filter test labels to only include transformed images\n",
    "test_labels_dict_incl = {}\n",
    "for (key, value) in test_labels_dict.items():\n",
    "    if key in partition['test']:\n",
    "        test_labels_dict_incl[key] = value\n",
    "\n",
    "# generate flattened dict of model task corresponding labels\n",
    "train_labels_dict_flat = util.select_label(train_labels_dict_incl, model_task)\n",
    "test_labels_dict_flat = util.select_label(test_labels_dict_incl, model_task)\n",
    "\n",
    "# encode categorical labels for classification tasks\n",
    "if model_task in ['Class', 'Subclass']:\n",
    "    y_train, y_test = util.encode_labels(train_labels_dict_flat, test_labels_dict_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into train and validation\n",
    "# TODO: random split errors when runing with existing false labels .npy\n",
    "partition['train'], y_train, partition['val'], y_val = util.train_val_split(partition['train'], y_train, val_split)\n",
    "print('# train imgs:', len(partition['train']), '- # val imgs:', len(partition['val']), '- # test imgs:', len(partition['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in false_ratio:\n",
    "    # create false train labels\n",
    "    # TODO: random split errors when runing with existing false labels .npy\n",
    "    y_train = util.make_false_labels(train_label_path, y_train, ratio, n_classes)\n",
    "\n",
    "    # define data loader parameters\n",
    "    params = {'dim': (resize_to[0],resize_to[1]),\n",
    "            'batch_size': batch_size,\n",
    "            'n_classes': n_classes,\n",
    "            'n_channels': 3,\n",
    "            'shuffle': True}\n",
    "\n",
    "    # load data\n",
    "    training_loader = dl.DataLoader(partition['train'], y_train, **params)\n",
    "    validation_loader = dl.DataLoader(partition['val'], y_val, **params)\n",
    "    test_loader = dl.DataLoader(partition['test'], y_test, **params)\n",
    "\n",
    "    # load models\n",
    "    basic_cnn = mdls.create_cnn_model(resize_to, n_classes, ratio)\n",
    "    resnet_cnn = mdls.create_resnet_model(resize_to, n_classes, ratio)\n",
    "    # yolo_cnn = mdls.create_yolo_model() # TODO: add yolo model\n",
    "    all_models = [resnet_cnn]\n",
    "\n",
    "    for model in all_models:\n",
    "        print(model.summary())\n",
    "        \n",
    "        # initialize logging\n",
    "        logdir = f'../logs/scalars/{model._name}/{datetime.now().strftime(\"%Y%m%d-%H%M\")}'\n",
    "        tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "        classReport_callback = cbs.class_report_cb()\n",
    "\n",
    "        # compile model\n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # train model with tensorboard and classification report logging\n",
    "        history = model.fit(x = training_loader,\n",
    "                                epochs = n_epochs,\n",
    "                                verbose = 2,\n",
    "                                callbacks=[classReport_callback,\n",
    "                                           tensorboard_callback],\n",
    "                                validation_data = validation_loader,\n",
    "                                use_multiprocessing = multiprocessing,\n",
    "                                workers = n_workers)\n",
    "        \n",
    "        # show test accuracy\n",
    "        score = model.evaluate(x = test_loader,\n",
    "                               callbacks=[tensorboard_callback],\n",
    "                               use_multiprocessing = multiprocessing,\n",
    "                               workers = n_workers,\n",
    "                               verbose = 0)\n",
    "\n",
    "        print(model._name, '- Test accuracy:', score[1])\n",
    "\n",
    "        model.save(f'../logs/models/{model._name}/{datetime.now().strftime(\"%Y%m%d-%H%M\")}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "785473fea46f9ac689a96b3810ed86c0f03e438ffd4c73e9fe241b9bd8d70125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('false-labels': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
