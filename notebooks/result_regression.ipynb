{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize logging\n",
    "eval_log_path = Path('../logs/result_log')\n",
    "if not os.path.exists(eval_log_path):\n",
    "    os.mkdir(eval_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ARCHIVED --\n",
    "# upload / download models and logs to tensorboard\n",
    "\n",
    "# copy relevant model metrics into seperate folder and upload to tensorboard dev\n",
    "# data_filter = 'test' # 'train', 'test' or 'validation'\n",
    "# model_filter = 'resnet'# 'basic' or 'resnet'\n",
    "# log_path = '../logs/scalars'\n",
    "# upload_path = '../tensorboard_upload/'\n",
    "# os.mkdir(upload_path)\n",
    "\n",
    "# for (dirpath, dirnames, filenames) in os.walk(log_path):\n",
    "#     # filter dirs\n",
    "#     if data_filter in dirpath and model_filter in dirpath:\n",
    "\n",
    "#         from_directory = dirpath\n",
    "#         to_directory = upload_path + dirpath[16:]\n",
    "\n",
    "#         # copy dir trees\n",
    "#         shutil.copytree(from_directory, to_directory)\n",
    "\n",
    "# # TODO: run in project root for upload\n",
    "# print(f'tensorboard dev upload --logdir=./tensorboard_upload/ --name FLE_{model_filter}_{data_filter}')\n",
    "\n",
    "# # remove copied dir trees\n",
    "# shutil.rmtree(upload_path)\n",
    "\n",
    "# # TODO: set tensorboard dev ID\n",
    "# experiment_ids = ['0m4NvD5jTxmSw3hr8SeJww', 'uLYWP18zQ1eMtDBiLHLHrg'] # [resnet, basic]\n",
    "# all_dfs = []\n",
    "\n",
    "# # download experiments\n",
    "# for ind, id in enumerate(experiment_ids):\n",
    "#     experiment = tb.data.experimental.ExperimentFromDev(id)\n",
    "#     df_temp = experiment.get_scalars()\n",
    "    \n",
    "#     print(f'{ind}: {df_temp.shape}')\n",
    "#     all_dfs.append(df_temp)\n",
    "\n",
    "# # combine experiments into single dataframe\n",
    "# df = all_dfs[0]\n",
    "\n",
    "# if len(all_dfs) > 1:\n",
    "#     for new_df in all_dfs[1:]:\n",
    "#         df = pd.concat([df, new_df])\n",
    "\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path and file filter\n",
    "report_log_path = '../logs/class_report'\n",
    "filter = 'class_report_19epoch.json'\n",
    "\n",
    "# define metrics to be extracted as list\n",
    "logs = [['accuracy']\n",
    "        , ['macro avg', 'precision']\n",
    "        , ['macro avg', 'recall']\n",
    "        , ['macro avg', 'f1-score']\n",
    "        , ['weighted avg', 'precision']\n",
    "        , ['weighted avg', 'recall']\n",
    "        , ['weighted avg', 'f1-score']\n",
    "]\n",
    "\n",
    "# init dataframe\n",
    "df_headers = ['run', 'model', 'ratio', 'classes', 'metric', 'value']\n",
    "df = pd.DataFrame(columns=df_headers)\n",
    "\n",
    "# iter through logs and get metrics from classification report jsons\n",
    "counter = 0\n",
    "for (dirpath, dirnames, filenames) in os.walk(report_log_path):\n",
    "\n",
    "    if filter in filenames:\n",
    "        counter += 1\n",
    "\n",
    "        file_path = dirpath+'/'+filter\n",
    "        class_report = json.load(open(file_path))\n",
    "\n",
    "        for log in logs:\n",
    "\n",
    "            run = dirpath.split('\\\\', 1)[-1].replace('\\\\', '_')\n",
    "            model = re.search('(^[a-z]{5,6})_', run).group(1)\n",
    "            ratio = float(re.search('(\\d{5})r', run).group(1)) / 100\n",
    "            classes = re.search('(\\d{1,2})c', run).group(1)\n",
    "            \n",
    "            if len(log) == 1:\n",
    "                metric = log[0]\n",
    "                value = class_report[log[0]]\n",
    "            elif len(log) == 2:\n",
    "                metric = log[0] + ' ' + log[1]\n",
    "                value = class_report[log[0]][log[1]]\n",
    "            \n",
    "            row = [\n",
    "                str(run),\n",
    "                str(model),\n",
    "                float(ratio),\n",
    "                str(classes),\n",
    "                str(metric),\n",
    "                float(value)\n",
    "            ]\n",
    "\n",
    "            df_len = len(df)\n",
    "            df.loc[df_len] = row\n",
    "\n",
    "print(f'{counter} classification reports found - df.shape {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wich training setups have less than 20 runs\n",
    "df_runs = df.groupby(['model', 'ratio', 'classes', 'metric']).count().reset_index()\n",
    "df_runs = df_runs.drop('value', axis=1).query('metric == \"accuracy\"')\n",
    "df_runs.query('run < 20').sort_values(by=['model', 'ratio', 'classes'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set regression data parameters\n",
    "regressand = 'accuracy' # regressand to use\n",
    "cnn = 'resnet' # model to filter for ('resnet', 'basic'), use None for all\n",
    "classes = 4 # classification task to filter for (4 or 14), use None for all\n",
    "\n",
    "use_delta = False # use regressand delta to 0 ratio model for regression\n",
    "# NOTE: set to False only if cnn & classes are set to None\n",
    "use_aggr = False # use aggregated data for each model + classes + ratio + metric\n",
    "\n",
    "# TODO: set data preparation parameters\n",
    "use_scaling = False # use scaling for regression\n",
    "zscore_threshold = 3 # zscore threshold for removing outlier based on regressand\n",
    "\n",
    "# file name depending on regression data parameters\n",
    "file_name = 'regr.json'\n",
    "if use_delta: file_name = 'delta_' + file_name\n",
    "file_name = f'{regressand}_' + file_name\n",
    "if cnn: file_name = cnn + 'CNN_' + file_name\n",
    "if classes: file_name = str(classes) + 'classes_' + file_name\n",
    "if use_aggr: file_name = 'aggr_' + file_name\n",
    "\n",
    "file_name = file_name.replace(' ', '_')\n",
    "print(f'Regression results will be saved to:\\n{file_name}')\n",
    "\n",
    "# define regression properties for logging\n",
    "regr_props = {}\n",
    "regr_props['properties'] = {}\n",
    "regr_props['properties']['name'] = file_name\n",
    "regr_props['properties']['type'] = 'regression'\n",
    "regr_props['properties']['regressand'] = regressand\n",
    "regr_props['properties']['model'] = cnn\n",
    "regr_props['properties']['classes'] = classes\n",
    "regr_props['properties']['delta'] = use_delta\n",
    "regr_props['properties']['aggretad_data'] = use_aggr\n",
    "regr_props['properties']['scaled_data'] = use_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataframe\n",
    "df_regression = df.query(f'metric == \"{regressand}\"')\n",
    "df_regression = df_regression.drop(['run'], axis=1)\n",
    "df_regression = df_regression.rename(columns={'value': regressand})\n",
    "\n",
    "# filter data for set parameter\n",
    "if cnn:\n",
    "    df_regression = df_regression.query(f'model == \"{cnn}\"')\n",
    "if classes:\n",
    "    df_regression = df_regression.query(f'classes == \"{classes}\"')\n",
    "if use_aggr:\n",
    "    df_regression = df_regression.groupby(['model', 'ratio', 'classes', 'metric']).mean().reset_index()\n",
    "\n",
    "# calc delta to baseline of average 0 ratio model + class setup\n",
    "df_tmp = df.groupby(['model', 'classes', 'ratio', 'metric']).mean().reset_index()\n",
    "df_regression[f'{regressand}_delta'] = df_regression.apply(lambda row:\n",
    "    row[regressand] - df_tmp.query(f'model==\"{row.model}\" & classes==\"{row.classes}\" & ratio==0.0 & metric==\"{row.metric}\"').iloc[0]['value'], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "df_regression.drop(['metric'], axis=1, inplace=True)\n",
    "df_regression.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(df_regression.shape)\n",
    "df_regression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df_regression[[regressand, f'{regressand}_delta', 'ratio']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(x='ratio', y=regressand, data=df_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter outlier of regressand (if data is not aggregated)\n",
    "if not use_aggr:\n",
    "    # create mask based on zscore threshold\n",
    "    mask = np.abs(stats.zscore(df_regression[regressand])) > zscore_threshold\n",
    "    print(f'{mask.sum()} rows filtered')\n",
    "\n",
    "    # apply mask\n",
    "    df_regression[regressand] = df_regression[regressand].mask(mask)\n",
    "\n",
    "    # drop outliers\n",
    "    df_regression.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get independent and dependent variables\n",
    "if use_delta:\n",
    "    X_df = df_regression[[f'{regressand}_delta']]\n",
    "else:\n",
    "    X_df = df_regression[[regressand]]\n",
    "y_df = df_regression['ratio']\n",
    "\n",
    "# calculate correlation (pearsonr and p-values)\n",
    "for column in X_df.columns:\n",
    "    r, p = stats.pearsonr(X_df[column], y_df)\n",
    "    print(f'{round(r, 4)} pearsonr, {p} p-value, Variable: {column}')\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                X_df\n",
    "                                                , y_df\n",
    "                                                , test_size=0.15\n",
    "                                                , stratify=y_df\n",
    "                                            )\n",
    "\n",
    "# scale X data\n",
    "if use_scaling:\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f'\\nX_train.shape {X_train.shape} - y_train.shape {y_train.shape}\\n\\\n",
    "X_test.shape {X_test.shape} - y_test.shape {y_test.shape}')\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize estimators and hyperparameter params for grid search\n",
    "regr_dummy = DummyRegressor()\n",
    "regr_dummy_params = {}\n",
    "regr_dummy_params['strategy'] = ['mean', 'median', 'quantile']\n",
    "regr_dummy_params['quantile'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "regr_linear = LinearRegression()\n",
    "regr_linear_params = {}\n",
    "\n",
    "regr_lasso = Lasso()\n",
    "regr_lasso_params = {}\n",
    "regr_lasso_params['max_iter'] = [10000]\n",
    "regr_lasso_params['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "regr_lasso_params['selection'] = ['random', 'cyclic']\n",
    "\n",
    "regr_elastic = ElasticNet()\n",
    "regr_elastic_params = {}\n",
    "regr_elastic_params['max_iter'] = [10000]\n",
    "regr_elastic_params['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "regr_elastic_params['l1_ratio'] = np.linspace(0.001, 0.999, 10)\n",
    "regr_elastic_params['selection'] = ['random', 'cyclic']\n",
    "\n",
    "regr_ridge = Ridge()\n",
    "regr_ridge_params = {}\n",
    "regr_ridge_params['max_iter'] = [10000]\n",
    "regr_ridge_params['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "regr_ridge_params['solver'] = [\n",
    "    'auto', 'svd', 'cholesky', 'lsqr'\n",
    "    , 'sparse_cg', 'sag', 'saga'\n",
    "]\n",
    "\n",
    "regr_svr = SVR()\n",
    "regr_svr_params = {}\n",
    "regr_svr_params['kernel'] = ['linear', 'poly', 'rbf'] # 'sigmoid'\n",
    "regr_svr_params['degree'] = [1, 2, 3]\n",
    "regr_svr_params['gamma'] = [1e-1, 1.0, 10.0]\n",
    "regr_svr_params['coef0'] = [0.0, 0.1, 0.3, 0.8]\n",
    "regr_svr_params['C'] = [1e-1, 1.0, 10.0]\n",
    "# regr_svr_params['epsilon'] = [1e-2, 1e-1, 1.0]\n",
    "\n",
    "regr_randomForest = RandomForestRegressor()\n",
    "regr_randomForest_params = {}\n",
    "regr_randomForest_params['n_estimators'] = [100, 200, 300, 400, 500]\n",
    "regr_randomForest_params['criterion'] = ['squared_error'] # 'absolute_error', 'poisson'\n",
    "regr_randomForest_params['max_depth'] = [1, 2, 3, 4, 5]\n",
    "\n",
    "regr_gradientBoost = GradientBoostingRegressor()\n",
    "regr_gradientBoost_params = {}\n",
    "regr_gradientBoost_params['loss'] = ['squared_error', 'huber']\n",
    "regr_gradientBoost_params['learning_rate'] = [0.1, 0.2, 0.5, 0.7]\n",
    "regr_gradientBoost_params['n_estimators'] = [100, 200, 400]\n",
    "# regr_gradientBoost_params['criterion'] = ['friedman_mse', 'squared_error', 'absolute_error']\n",
    "regr_gradientBoost_params['max_depth'] = [1, 2, 4]\n",
    "regr_gradientBoost_params['alpha'] = [0.1, 0.5, 0.9]\n",
    "\n",
    "models_and_params = [\n",
    "    [regr_dummy, regr_dummy_params]\n",
    "    , [regr_linear, regr_linear_params]\n",
    "    , [regr_lasso, regr_lasso_params]\n",
    "    , [regr_elastic, regr_elastic_params]\n",
    "    , [regr_ridge, regr_ridge_params]\n",
    "    , [regr_svr, regr_svr_params]\n",
    "    , [regr_randomForest, regr_randomForest_params]\n",
    "    , [regr_gradientBoost, regr_gradientBoost_params]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# iterate over models and hyperparameters and train\n",
    "regr_props['best_model_params'] = {}\n",
    "for model, params in models_and_params:\n",
    "    model_name = f'{str(model)[:-2]}'\n",
    "\n",
    "    # run gridsearch\n",
    "    print('--------')\n",
    "    start = time.time()\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        model\n",
    "        , params\n",
    "        # , n_iter=2\n",
    "        , n_jobs=-1\n",
    "        , cv=5\n",
    "        , scoring='r2'\n",
    "        , verbose=1\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f'{model_name}'\\\n",
    "        f' - Training time elapsed: {round((end - start) / 60, 3)} min'\\\n",
    "        f' - Best score: {round(gs.best_score_, 3)}'\n",
    "    )\n",
    "\n",
    "    # add best params + score per model to dict for logging\n",
    "    regr_props['best_model_params'][model_name] = {\n",
    "        'train_score': gs.best_score_,\n",
    "        'test_score': gs.score(X_test, y_test),\n",
    "        'best_params': gs.best_params_\n",
    "    }\n",
    "\n",
    "# save best params per model\n",
    "with open(f'{eval_log_path}/{file_name}', 'w') as f:\n",
    "    json.dump(regr_props, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c041ee5eca0bebec98353fb08f3f656fe2677ac141976eeebd4959b2f854b27b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('false-labels')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
